# ğŸ§  Local LLM Inference API (FastAPI + llama.cpp)

A production-grade, containerized FastAPI backend for running quantized GGUF language models via `llama.cpp`.  
Supports local inference with models like Mistral, LLaMA 3, MythoMax, and Chronos Hermes.

This project forms the backend layer of a broader system for creative tools, research assistants, and experimental RAG-based storytelling agents â€” all powered locally.

---

## ğŸš€ Features

- ğŸ”’ **Private, local inference** â€“ no API keys, no 3rd-party requests
- ğŸ³ **Dockerized** with Makefile automation
- âš¡ **FastAPI** backend with route modularization
- ğŸ§ª **Test suite** using `pytest` and `unittest.mock`
- âš™ï¸ **Config system** using `.env` + `config.yml`
- ğŸ“¦ **Supports quantized .gguf models** via `llama-cpp-python`

---

## ğŸ³ Usage

### ğŸ”§ Setup

```bash
cp .env.example .env
make up        # Build and run the container
```

### âœ… Test

```bash
make test-local    # Runs tests locally
make test          # Runs tests in Docker
```

### ğŸ›‘ Stop

```bash
make down
```

---

## ğŸ“‹ Config

Config values come from both:

- `config.yml` â€“ Model path, API host/port, etc.
- `.env` â€“ Used for environment-level overrides

---

## ğŸ§ª Test Coverage

- âœ… `run_prompt()` logic via mock injection
- âœ… FastAPI route: `/infer`
- ğŸ”œ More integration tests planned
- ğŸ”œ Add streaming support

---
## ğŸ”­ Roadmap

A living development plan â€” checkmarks mark what's done, and everything else is on deck.

- [x] Clone Dockerized FastAPI boilerplate
- [x] Choose initial inference model & download `.gguf`
- [x] Load the model via `llama-cpp`
- [x] Connect model to FastAPI inference route
- [x] Validate functionality with test prompts

### Up Next

- [ ] ğŸ–¼ Build Tauri-based desktop UI
- [ ] ğŸ“š Add RAG pipeline (chunking, embeddings, vector DB)
- [ ] ğŸŒ Research self-hosting / remote deployment options
- [ ] ğŸš€ Deploy project to selected hosting platform
- [ ] ğŸ” Implement hot-reload dev server (Uvicorn + volume mounts)
- [ ] ğŸ›  Refactor Dockerfile for multi-stage (dev/prod)
- [ ] ğŸ§ª Split test/dev containers with `docker-compose.override.yml`

### ğŸ›£ï¸ Down the Road

- [ ] ğŸ” Add Auth Layer
    - Options: API key, OAuth2, JWT, or local password-protected endpoints
    - Might pair well with user/session context for future multi-user RAG
- [ ] ğŸ”„ Model Switching Mechanism
    - Dynamically load different .gguf models via config or runtime param
- [ ] ğŸŒŠ Streaming Responses
    - Add `stream=True` support for real-time token generation in `/infer/`
- [ ] âœ… Test Coverage Expansion
    - Auth endpoints, error handling, config fallbacks
---

## ğŸ§  Target Models for Local Inference

| Model | Strength | Notes | Download |
|-------|----------|-------|----------|
| âœ… **Mistral-7B-Instruct** | Fast, lightweight | Perfect for bootstrapping | [mistral-7b-instruct.Q4_K_M.gguf](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF) |
| â³ **LLaMA 3 8B** | Balanced & powerful | Step up once base is stable | [llama-3-8B-Instruct.Q4_K_M.gguf](https://huggingface.co/TheBloke/Llama-3-8B-Instruct-GGUF) |
| ğŸŒ€ **MythoMax 13B** | Imaginative & surreal | Great for narrative/story gen | [MythoMax-L2-13B.Q4_K_M.gguf](https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF) |
| â± **Chronos Hermes 13B** | Time-sensitive & verbose | Ideal for longform + RAG fusion | [Chronos-Hermes-13B.Q4_K_M.gguf](https://huggingface.co/TheBloke/Chronos-Hermes-13B-GGUF) |

---

## ğŸ“„ License

MIT â€“ free for personal or commercial use.